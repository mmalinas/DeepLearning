{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save_breastcancerimages_h5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGbRTmqwo0MMtgmUO4SzTB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmalinas/DeepLearning/blob/master/save_breastcancerimages_h5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlzsNNK8tU_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "from random import shuffle\n",
        "import h5py\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-93vSshtbz-",
        "colab_type": "code",
        "outputId": "2c908b8d-f84a-47bb-f75d-7e2e50f50437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_9wGP9QtqVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_and_write_data_into_h5_file(dest_filepath, filepaths_list, dataset_name, n_px, n_channels = 3):\n",
        "    \n",
        "    '''\n",
        "        This function converts images to numpy arrays and writes the array data into a h5 file.\n",
        "        \n",
        "        dest_filepath - the name of the file with full path that is being created\n",
        "        filepaths_list - source image file paths which is being converted to numpy arrays\n",
        "        n_px - number of pixels - will be used as image's height and width\n",
        "        n_channels - 3 for rgb\n",
        "    '''\n",
        "    \n",
        "    data_shape = (len(filepaths_list), n_px * n_px * n_channels)\n",
        "    \n",
        "    with h5py.File(dest_filepath, 'a') as f:\n",
        "        \n",
        "        f.require_dataset(dataset_name, data_shape, np.float32)\n",
        "        \n",
        "        for i in range(len(filepaths_list)):\n",
        "            #if (i+1) % 512 == 0:\n",
        "            #    print('{}/{} files converted'.format((i+1), len(filepaths_list)))\n",
        "            filepath = filepaths_list[i]\n",
        "            img = cv2.imread(filepath)\n",
        "            img = cv2.resize(img, (n_px, n_px), interpolation=cv2.INTER_CUBIC)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            #Normalize the image - convert the each pixel value between 0 and 1\n",
        "            img = img / 255\n",
        "            #Reshape the image - roll it up into a column vector\n",
        "            img = img.ravel()\n",
        "            \n",
        "            #img[None] makes it a proper array instead of rank 1 array\n",
        "            f[dataset_name][i, ...] = img[None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by3YJoz-uJ3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_labels_into_h5_file(dest_filepath, labels):\n",
        "    \n",
        "    dataset_name = \"labels\"\n",
        "    \n",
        "    with h5py.File(dest_filepath, 'a') as f:\n",
        "        f.create_dataset(dataset_name, (len(labels),), np.int8)\n",
        "        f[dataset_name][...] = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjOkd98ruQT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_images_to_data_in_h5_file(src_img_filepath_pattern, dest_h5_file_path, n_px, \n",
        "                                      n_channels = 3):\n",
        "    \n",
        "    #Returns a list of filepaths matching the pattern given as parameter\n",
        "    src_filepaths = glob.glob(src_img_filepath_pattern, recursive=True)\n",
        "    \n",
        "    #Create Labels based upon the substring contained in the filename\n",
        "    labels = [0 if 'class0' in filepath else 1 for filepath in src_filepaths]\n",
        "    \n",
        "    #The zip(source_filepaths, labels) combines each element of source_filepaths list \n",
        "    #with each element of labels list forming a pair (tuple). t is the list which contains these tuples\n",
        "    \n",
        "    t = list(zip(src_filepaths, labels))\n",
        "\n",
        "    class0_tuples = []\n",
        "    class1_tuples = []\n",
        "\n",
        "    #separate class 0 and class 1 tuples\n",
        "    for t_tuple in t:\n",
        "      if 'class0' in t_tuple[0]:\n",
        "        class0_tuples.append(t_tuple)\n",
        "      else:\n",
        "        class1_tuples.append(t_tuple)\n",
        "\n",
        "    shuffle(class0_tuples)\n",
        "    shuffle(class1_tuples)\n",
        "    class0_tuples_downsample = class0_tuples[0:len(class1_tuples)]\n",
        "\n",
        "    training_final_index = math.ceil(0.8*len(class1_tuples))\n",
        "    validation_final_index = training_final_index + math.ceil(0.1*len(class1_tuples))\n",
        "    \n",
        "    training_class0 = class0_tuples[0:training_final_index]\n",
        "    validation_class0 = class0_tuples[training_final_index:validation_final_index]\n",
        "    test_class0 = class0_tuples[validation_final_index:]\n",
        "\n",
        "    training_class1 = class1_tuples[0:training_final_index]\n",
        "    validation_class1 = class1_tuples[training_final_index:validation_final_index]\n",
        "    test_class1 = class1_tuples[validation_final_index:]\n",
        "\n",
        "    print('Creating file training_class0')\n",
        "    dest_file_path_training_class0 = dest_h5_file_path + 'training_class0' + '.h5'\n",
        "    src_filepaths_training_class0, labels_training_class0 = zip(*training_class0)\n",
        "    normalize_and_write_data_into_h5_file(dest_file_path_training_class0, src_filepaths_training_class0, 'training_class0', n_px)\n",
        "    write_labels_into_h5_file(dest_file_path_training_class0, labels_training_class0)\n",
        "\n",
        "    print('Creating file validation_class0')\n",
        "    dest_file_path_validation_class0 = dest_h5_file_path + 'validation_class0' + '.h5'\n",
        "    src_filepaths_validation_class0, labels_validation_class0 = zip(*validation_class0)\n",
        "    normalize_and_write_data_into_h5_file(dest_file_path_validation_class0, src_filepaths_validation_class0, 'validation_class0', n_px)\n",
        "    write_labels_into_h5_file(dest_file_path_validation_class0, labels_validation_class0)\n",
        "\n",
        "    print('Creating file test_class0')\n",
        "    dest_file_path_test_class0 = dest_h5_file_path + 'test_class0' + '.h5'\n",
        "    src_filepaths_test_class0, labels_test_class0 = zip(*test_class0)\n",
        "    normalize_and_write_data_into_h5_file(dest_file_path_test_class0, src_filepaths_test_class0, 'test_class0', n_px)\n",
        "    write_labels_into_h5_file(dest_file_path_test_class0, labels_test_class0)\n",
        "\n",
        "    print('Creating file training_class1')\n",
        "    dest_file_path_training_class1 = dest_h5_file_path + 'training_class1' + '.h5'\n",
        "    src_filepaths_training_class1, labels_training_class1 = zip(*training_class1)\n",
        "    normalize_and_write_data_into_h5_file(dest_file_path_training_class1, src_filepaths_training_class1, 'training_class1', n_px)\n",
        "    write_labels_into_h5_file(dest_file_path_training_class1, labels_training_class1)\n",
        "\n",
        "    print('Creating file validation_class1')\n",
        "    dest_file_path_validation_class1 = dest_h5_file_path + 'validation_class1' + '.h5'\n",
        "    src_filepaths_validation_class1, labels_validation_class1 = zip(*validation_class1)\n",
        "    normalize_and_write_data_into_h5_file(dest_file_path_validation_class1, src_filepaths_validation_class1, 'validation_class1', n_px)\n",
        "    write_labels_into_h5_file(dest_file_path_validation_class1, labels_validation_class1)\n",
        "\n",
        "    print('Creating file test_class1')\n",
        "    dest_file_path_test_class1 = dest_h5_file_path + 'test_class1' + '.h5'\n",
        "    src_filepaths_test_class1, labels_test_class1 = zip(*test_class1)\n",
        "    normalize_and_write_data_into_h5_file(dest_file_path_test_class1, src_filepaths_test_class1, 'test_class1', n_px)\n",
        "    write_labels_into_h5_file(dest_file_path_test_class1, labels_test_class1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzTChI7HulP4",
        "colab_type": "code",
        "outputId": "e141c6a8-543c-495a-b083-814a70d5681d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "src_filepath_pattern = '/content/drive/My Drive/breast-histopathology-images/8*/*/*'\n",
        "dest_filepath = '/content/drive/My Drive/breast-normalized-dataset_'\n",
        "n_px = 50\n",
        "n_channels = 3\n",
        "\n",
        "tic = time.process_time()\n",
        "convert_images_to_data_in_h5_file(src_filepath_pattern, dest_filepath, n_px, n_channels)\n",
        "toc = time.process_time()\n",
        "print('Time taken for creating the h5 file is', (toc-tic)*1000, 'ms')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating file training_class0\n",
            "Creating file validation_class0\n",
            "Creating file test_class0\n",
            "Creating file training_class1\n",
            "Creating file validation_class1\n",
            "Creating file test_class1\n",
            "Time taken for creating the h5 file is 29954.363139999998 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}